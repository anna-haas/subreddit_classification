{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "cv_df = pd.read_csv('../data/count_vec.csv')\n",
    "tfidf_df = pd.read_csv('../data/tfidf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_serious</th>\n",
       "      <th>sent_compound</th>\n",
       "      <th>sent_neg</th>\n",
       "      <th>sent_neu</th>\n",
       "      <th>sent_pos</th>\n",
       "      <th>char_count</th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>001</th>\n",
       "      <th>00pm</th>\n",
       "      <th>...</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zen</th>\n",
       "      <th>zeppelin</th>\n",
       "      <th>zero</th>\n",
       "      <th>zip</th>\n",
       "      <th>zoloft</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoned</th>\n",
       "      <th>zoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.661</td>\n",
       "      <td>0.273</td>\n",
       "      <td>4862</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.293</td>\n",
       "      <td>6225</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.9195</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.112</td>\n",
       "      <td>1105</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.9943</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.709</td>\n",
       "      <td>0.071</td>\n",
       "      <td>1425</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.9988</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.284</td>\n",
       "      <td>2104</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 11251 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_serious  sent_compound  sent_neg  sent_neu  sent_pos  char_count  00  \\\n",
       "0           0         0.9997     0.066     0.661     0.273        4862   0   \n",
       "1           0         0.9999     0.057     0.649     0.293        6225   0   \n",
       "2           0         0.9195     0.068     0.820     0.112        1105   0   \n",
       "3           0        -0.9943     0.221     0.709     0.071        1425   0   \n",
       "4           0         0.9988     0.080     0.636     0.284        2104   0   \n",
       "\n",
       "   000  001  00pm  ...  zealand  zen  zeppelin  zero  zip  zoloft  zombie  \\\n",
       "0    0    0     0  ...        0    0         0     0    0       0       0   \n",
       "1    0    0     0  ...        0    0         0     0    0       0       0   \n",
       "2    0    0     0  ...        0    0         0     0    0       0       0   \n",
       "3    0    0     0  ...        0    0         0     0    0       0       0   \n",
       "4    0    0     0  ...        0    0         0     0    0       0       0   \n",
       "\n",
       "   zone  zoned  zoo  \n",
       "0     0      0    0  \n",
       "1     0      0    0  \n",
       "2     0      0    0  \n",
       "3     1      0    0  \n",
       "4     0      0    0  \n",
       "\n",
       "[5 rows x 11251 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1694, 11251)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_serious</th>\n",
       "      <th>sent_compound</th>\n",
       "      <th>sent_neg</th>\n",
       "      <th>sent_neu</th>\n",
       "      <th>sent_pos</th>\n",
       "      <th>char_count</th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>...</th>\n",
       "      <th>yr</th>\n",
       "      <th>yt</th>\n",
       "      <th>yummy</th>\n",
       "      <th>yup</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zero</th>\n",
       "      <th>zip</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.661</td>\n",
       "      <td>0.273</td>\n",
       "      <td>4862</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.293</td>\n",
       "      <td>6225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.9195</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.112</td>\n",
       "      <td>1105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.9943</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.709</td>\n",
       "      <td>0.071</td>\n",
       "      <td>1425</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.085366</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.9988</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.284</td>\n",
       "      <td>2104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7926 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_serious  sent_compound  sent_neg  sent_neu  sent_pos  char_count   00  \\\n",
       "0           0         0.9997     0.066     0.661     0.273        4862  0.0   \n",
       "1           0         0.9999     0.057     0.649     0.293        6225  0.0   \n",
       "2           0         0.9195     0.068     0.820     0.112        1105  0.0   \n",
       "3           0        -0.9943     0.221     0.709     0.071        1425  0.0   \n",
       "4           0         0.9988     0.080     0.636     0.284        2104  0.0   \n",
       "\n",
       "   000   01   02  ...   yr   yt  yummy  yup  zealand  zero  zip  zombie  \\\n",
       "0  0.0  0.0  0.0  ...  0.0  0.0    0.0  0.0      0.0   0.0  0.0     0.0   \n",
       "1  0.0  0.0  0.0  ...  0.0  0.0    0.0  0.0      0.0   0.0  0.0     0.0   \n",
       "2  0.0  0.0  0.0  ...  0.0  0.0    0.0  0.0      0.0   0.0  0.0     0.0   \n",
       "3  0.0  0.0  0.0  ...  0.0  0.0    0.0  0.0      0.0   0.0  0.0     0.0   \n",
       "4  0.0  0.0  0.0  ...  0.0  0.0    0.0  0.0      0.0   0.0  0.0     0.0   \n",
       "\n",
       "       zone  zoo  \n",
       "0  0.000000  0.0  \n",
       "1  0.000000  0.0  \n",
       "2  0.000000  0.0  \n",
       "3  0.085366  0.0  \n",
       "4  0.000000  0.0  \n",
       "\n",
       "[5 rows x 7926 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1694, 7926)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Various Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: Confusion Matrix Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_results(y_test, predictions):\n",
    "    # Create confustion matrix and conver to dataframe\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    print('-------------CONFUSION MATRIX---------------')\n",
    "    print(pd.DataFrame(cm, \n",
    "                       columns=['pred neg', 'pred pos'], \n",
    "                       index = ['actual neg', 'actual pos']))\n",
    "    \n",
    "    # Results\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "    print(\"\\nTrue Negatives: %s\" % tn)\n",
    "    print(\"False Positives: %s\" % fp)\n",
    "    print(\"False Negatives: %s\" % fn)\n",
    "    print(\"True Positives: %s\" % tp)\n",
    "    print('------------------METRICS-------------------')\n",
    "    print(f'Sensativity: {round(tp / (tp + fn),4)}')\n",
    "    print(f'Specificity: {round(tn / (tn + fp),4)}\\n\\n')\n",
    "    # Other metrics?? missclassification? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisitic_reg(df):\n",
    "    print('*********** LOGISTIC REGRESSION MODEL ************\\n')\n",
    "    features = [column for column in df.columns if column != 'is_serious']\n",
    "    features.remove('sent_compound')\n",
    "    features.remove('sent_neu')\n",
    "    features.remove('char_count') # giving better scores\n",
    "    X = df[features]\n",
    "    y = df['is_serious']\n",
    "    \n",
    "    # Train Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify = y)\n",
    "    \n",
    "    # Instantiate Logistic Regression model\n",
    "    lr = LogisticRegression(solver='lbfgs')\n",
    "    \n",
    "    # Fit the model\n",
    "    lr.fit(X_train, y_train)\n",
    "    \n",
    "    # Score the model\n",
    "    print('--------------MODEL EVALUTATION---------------')\n",
    "    print('Cross Val Score: ', cross_val_score(lr, X_train, y_train, cv = 5).mean())\n",
    "    print('Train Score:     ', lr.score(X_train, y_train))\n",
    "    print('Test Score:      ', lr.score(X_test, y_test))\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = lr.predict(X_test)\n",
    "    \n",
    "    # Confusion matrix and measures\n",
    "    confusion_matrix_results(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** LOGISTIC REGRESSION MODEL ************\n",
      "\n",
      "--------------MODEL EVALUTATION---------------\n",
      "Cross Val Score:  0.8504352952160914\n",
      "Train Score:      0.9409448818897638\n",
      "Test Score:       0.8136792452830188\n",
      "-------------CONFUSION MATRIX---------------\n",
      "            pred neg  pred pos\n",
      "actual neg       163        36\n",
      "actual pos        43       182\n",
      "\n",
      "True Negatives: 163\n",
      "False Positives: 36\n",
      "False Negatives: 43\n",
      "True Positives: 182\n",
      "------------------METRICS-------------------\n",
      "Sensativity: 0.8089\n",
      "Specificity: 0.8191\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tf-Idf\n",
    "logisitic_reg(tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** LOGISTIC REGRESSION MODEL ************\n",
      "\n",
      "--------------MODEL EVALUTATION---------------\n",
      "Cross Val Score:  0.8291595984819498\n",
      "Train Score:      1.0\n",
      "Test Score:       0.7806603773584906\n",
      "-------------CONFUSION MATRIX---------------\n",
      "            pred neg  pred pos\n",
      "actual neg       161        38\n",
      "actual pos        55       170\n",
      "\n",
      "True Negatives: 161\n",
      "False Positives: 38\n",
      "False Negatives: 55\n",
      "True Positives: 170\n",
      "------------------METRICS-------------------\n",
      "Sensativity: 0.7556\n",
      "Specificity: 0.809\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count Vec\n",
    "logisitic_reg(cv_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- Best Cross Val Score: 0.85 -- Logistic Regression with Tf-Idf\n",
    "- Using Tf-Idf leads to a better score (higher cross val, higher sensativity and higher sepcificity) than Count Vectorize\n",
    "- The Tf-Idf Logistic Regression model is still overfit (train = 0.94, Test = 0.813)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(df):\n",
    "    print('*********** KNN MODEL ************\\n')\n",
    "    features = [column for column in df.columns if column != 'is_serious']\n",
    "    features.remove('sent_compound')\n",
    "    features.remove('sent_neu')\n",
    "    features.remove('char_count')\n",
    "    \n",
    "    X = df[features]\n",
    "    y = df['is_serious']\n",
    "    \n",
    "    # Train Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify = y)\n",
    "\n",
    "    # Instantiate KNN Model\n",
    "    knn = KNeighborsClassifier()\n",
    "    \n",
    "    # Fit the model\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Score the model\n",
    "    print('--------------MODEL EVALUTATION---------------')\n",
    "    print('Cross Val Score: ', cross_val_score(knn, X_train, y_train, cv = 5).mean())\n",
    "    print('Train Score:     ', knn.score(X_train, y_train))\n",
    "    print('Test Score:      ', knn.score(X_test, y_test))\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = knn.predict(X_test)\n",
    "    \n",
    "    # Confusion matrix and measures\n",
    "    confusion_matrix_results(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** KNN MODEL ************\n",
      "\n",
      "--------------MODEL EVALUTATION---------------\n",
      "Cross Val Score:  0.7386413829171145\n",
      "Train Score:      0.8047244094488188\n",
      "Test Score:       0.7216981132075472\n",
      "-------------CONFUSION MATRIX---------------\n",
      "            pred neg  pred pos\n",
      "actual neg       106        93\n",
      "actual pos        25       200\n",
      "\n",
      "True Negatives: 106\n",
      "False Positives: 93\n",
      "False Negatives: 25\n",
      "True Positives: 200\n",
      "------------------METRICS-------------------\n",
      "Sensativity: 0.8889\n",
      "Specificity: 0.5327\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tf-Idf\n",
    "knn(tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** KNN MODEL ************\n",
      "\n",
      "--------------MODEL EVALUTATION---------------\n",
      "Cross Val Score:  0.6834420854333455\n",
      "Train Score:      0.8078740157480315\n",
      "Test Score:       0.660377358490566\n",
      "-------------CONFUSION MATRIX---------------\n",
      "            pred neg  pred pos\n",
      "actual neg       189        10\n",
      "actual pos       134        91\n",
      "\n",
      "True Negatives: 189\n",
      "False Positives: 10\n",
      "False Negatives: 134\n",
      "True Positives: 91\n",
      "------------------METRICS-------------------\n",
      "Sensativity: 0.4044\n",
      "Specificity: 0.9497\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count Vec\n",
    "knn(cv_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- Best Cross Val Score: 0.73 -- KNN with Tf-Idf\n",
    "- Tf-Idf performs better thant Count Vec (based on cross val score)\n",
    "- There is a large difference in the Sensativity metric...tf-idf: 0.889, cross vec: 0.404\n",
    "    - Tf-Idf really helps reduce false negatives (134 to 25)\n",
    "- There is a large difference in the Specificity metric...tf-idf: 0.532, cross vec: 0.949\n",
    "    - Cross Vec really helps reduce false positive (93 to 10)\n",
    "- The Tf-Idk model us less overfit (still overfit though) than the Cross Vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DECISION TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree(df):\n",
    "    print('*********** DECISION TREE MODEL ************\\n')\n",
    "    features = [column for column in df.columns if column != 'is_serious']\n",
    "    features.remove('sent_compound')\n",
    "    features.remove('sent_neu')\n",
    "    features.remove('char_count')\n",
    "    X = df[features]\n",
    "    y = df['is_serious']\n",
    "    \n",
    "    # Train Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify = y)\n",
    "    \n",
    "    # Instantiate DecisionTree Model\n",
    "    dt = DecisionTreeClassifier(random_state=45,\n",
    "                                max_depth = 7, \n",
    "                                min_samples_split = 15, \n",
    "                                min_samples_leaf = 6)\n",
    "    # Fit the model\n",
    "    dt.fit(X_train, y_train)\n",
    "    \n",
    "    # Score the model/Evaluate Model\n",
    "    print('--------------MODEL EVALUTATION---------------')\n",
    "    print('Cross Val Score: ', cross_val_score(dt, X_train, y_train, cv = 5).mean())\n",
    "    print('Train Score:     ', dt.score(X_train, y_train))\n",
    "    print('Test Score:      ', dt.score(X_test, y_test))\n",
    " \n",
    "    # Get predictions\n",
    "    predictions = dt.predict(X_test)\n",
    "    \n",
    "    # Confusion matrix and measures\n",
    "    confusion_matrix_results(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** DECISION TREE MODEL ************\n",
      "\n",
      "--------------MODEL EVALUTATION---------------\n",
      "Cross Val Score:  0.6984245743985559\n",
      "Train Score:      0.8653543307086614\n",
      "Test Score:       0.6981132075471698\n",
      "-------------CONFUSION MATRIX---------------\n",
      "            pred neg  pred pos\n",
      "actual neg       138        61\n",
      "actual pos        67       158\n",
      "\n",
      "True Negatives: 138\n",
      "False Positives: 61\n",
      "False Negatives: 67\n",
      "True Positives: 158\n",
      "------------------METRICS-------------------\n",
      "Sensativity: 0.7022\n",
      "Specificity: 0.6935\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tf-Idf\n",
    "decision_tree(tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** DECISION TREE MODEL ************\n",
      "\n",
      "--------------MODEL EVALUTATION---------------\n",
      "Cross Val Score:  0.7173316222010262\n",
      "Train Score:      0.8551181102362204\n",
      "Test Score:       0.6816037735849056\n",
      "-------------CONFUSION MATRIX---------------\n",
      "            pred neg  pred pos\n",
      "actual neg       160        39\n",
      "actual pos        96       129\n",
      "\n",
      "True Negatives: 160\n",
      "False Positives: 39\n",
      "False Negatives: 96\n",
      "True Positives: 129\n",
      "------------------METRICS-------------------\n",
      "Sensativity: 0.5733\n",
      "Specificity: 0.804\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count Vec\n",
    "decision_tree(cv_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- Best Cross Val Score: 0.71 -- Decision Tree with Cross Vec\n",
    "- Cross performs better thant Tf-Idf (based on cross val score)...unlike the previous two models\n",
    "- Tf-Idf improves sensativity while Count Vec improves sepcificity (like above)\n",
    "- Tf-Idf has a much more evenly distributed number of false positive and false negatives than Cross Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(df):\n",
    "    print('*********** RANDOM FOREST MODEL ************\\n')\n",
    "    features = [column for column in df.columns if column != 'is_serious']\n",
    "    features.remove('sent_compound')\n",
    "    features.remove('sent_neu')\n",
    "    features.remove('char_count')\n",
    "    X = df[features]\n",
    "    y = df['is_serious']\n",
    "    \n",
    "    # Train Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify = y)\n",
    "    \n",
    "    # Instantiate KNN Model\n",
    "    rf = RandomForestClassifier(n_estimators=10, \n",
    "                                max_depth = None, \n",
    "                                max_features = 'auto')\n",
    "    \n",
    "    # Fit the model\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Score the model\n",
    "    print('--------------MODEL EVALUTATION---------------')\n",
    "    print('Cross Val Score: ', cross_val_score(rf, X_train, y_train, cv = 5).mean())\n",
    "    print('Train Score:     ', rf.score(X_train, y_train))\n",
    "    print('Test Score:      ', rf.score(X_test, y_test))\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = rf.predict(X_test)\n",
    "    \n",
    "    # Confusion matrix and measures\n",
    "    confusion_matrix_results(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** RANDOM FOREST MODEL ************\n",
      "\n",
      "--------------MODEL EVALUTATION---------------\n",
      "Cross Val Score:  0.7614733312951086\n",
      "Train Score:      0.989763779527559\n",
      "Test Score:       0.7264150943396226\n",
      "-------------CONFUSION MATRIX---------------\n",
      "            pred neg  pred pos\n",
      "actual neg       159        40\n",
      "actual pos        76       149\n",
      "\n",
      "True Negatives: 159\n",
      "False Positives: 40\n",
      "False Negatives: 76\n",
      "True Positives: 149\n",
      "------------------METRICS-------------------\n",
      "Sensativity: 0.6622\n",
      "Specificity: 0.799\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tf-Idf\n",
    "random_forest(tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** RANDOM FOREST MODEL ************\n",
      "\n",
      "--------------MODEL EVALUTATION---------------\n",
      "Cross Val Score:  0.7662442049428778\n",
      "Train Score:      0.9929133858267717\n",
      "Test Score:       0.7311320754716981\n",
      "-------------CONFUSION MATRIX---------------\n",
      "            pred neg  pred pos\n",
      "actual neg       159        40\n",
      "actual pos        74       151\n",
      "\n",
      "True Negatives: 159\n",
      "False Positives: 40\n",
      "False Negatives: 74\n",
      "True Positives: 151\n",
      "------------------METRICS-------------------\n",
      "Sensativity: 0.6711\n",
      "Specificity: 0.799\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count Vec\n",
    "random_forest(cv_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- Best Cross Val Score: 0.76 -- Random Forest with Cross Vec\n",
    "- Cross performs better thant Tf-Idf (based on cross val score)\n",
    "- Both models have a specificity metric around 82-83%\n",
    "- The Corss Vec model has a slighly better senstitiviy metric (less false negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXTRA TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extra_trees(df):\n",
    "    print('*********** EXTRA TREES MODEL ************\\n')\n",
    "    features = [column for column in df.columns if column != 'is_serious']\n",
    "    features.remove('sent_compound')\n",
    "    features.remove('sent_neu')\n",
    "    features.remove('char_count')\n",
    "    X = df[features]\n",
    "    y = df['is_serious']\n",
    "    \n",
    "    # Train Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify = y)\n",
    "    \n",
    "    # Instantiate KNN Model\n",
    "    rf = RandomForestClassifier(n_estimators=10, \n",
    "                                max_depth = None, \n",
    "                                max_features = 'auto')\n",
    "    \n",
    "    # Train Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify = y)\n",
    "    \n",
    "    # Instantiate KNN Model\n",
    "    et = ExtraTreesClassifier(n_estimators=10)\n",
    "    \n",
    "    # Fit the model\n",
    "    et.fit(X_train, y_train)\n",
    "    \n",
    "    # Score the model\n",
    "    print('--------------MODEL EVALUTATION---------------')\n",
    "    print('Cross Val Score: ', cross_val_score(et, X_train, y_train, cv = 5).mean())\n",
    "    print('Train Score:     ', et.score(X_train, y_train))\n",
    "    print('Test Score:      ', et.score(X_test, y_test))\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = et.predict(X_test)\n",
    "    \n",
    "    # Confusion matrix and measures\n",
    "    confusion_matrix_results(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** EXTRA TREES MODEL ************\n",
      "\n",
      "--------------MODEL EVALUTATION---------------\n",
      "Cross Val Score:  0.7763968337949851\n",
      "Train Score:      1.0\n",
      "Test Score:       0.7452830188679245\n",
      "-------------CONFUSION MATRIX---------------\n",
      "            pred neg  pred pos\n",
      "actual neg       154        45\n",
      "actual pos        63       162\n",
      "\n",
      "True Negatives: 154\n",
      "False Positives: 45\n",
      "False Negatives: 63\n",
      "True Positives: 162\n",
      "------------------METRICS-------------------\n",
      "Sensativity: 0.72\n",
      "Specificity: 0.7739\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tf-idf\n",
    "extra_trees(tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** EXTRA TREES MODEL ************\n",
      "\n",
      "--------------MODEL EVALUTATION---------------\n",
      "Cross Val Score:  0.7629922602385699\n",
      "Train Score:      1.0\n",
      "Test Score:       0.7523584905660378\n",
      "-------------CONFUSION MATRIX---------------\n",
      "            pred neg  pred pos\n",
      "actual neg       168        31\n",
      "actual pos        74       151\n",
      "\n",
      "True Negatives: 168\n",
      "False Positives: 31\n",
      "False Negatives: 74\n",
      "True Positives: 151\n",
      "------------------METRICS-------------------\n",
      "Sensativity: 0.6711\n",
      "Specificity: 0.8442\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count Vec\n",
    "extra_trees(cv_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- Best Cross Val Score: 0.759 -- Extra Tress with Tf-Idf\n",
    "- Both models have very similar cross val scores (0.75)\n",
    "- The Tf-Idf does a better job predicting less false positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 3 Models\n",
    "1. Logistic Regression uing Tf-Idf (cv: 0.8504352952160914)\n",
    "2. Logistic Regression uing Cross Vectorization (cv: 0.8291595984819498)\n",
    "3. Random Forest using Cross Vectorization (cv: 0.7622948456716102)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression using Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set X and y\n",
    "features = [column for column in tfidf_df.columns if column != 'is_serious']\n",
    "features.remove('sent_compound')\n",
    "features.remove('sent_neu')\n",
    "features.remove('char_count')\n",
    "X = tfidf_df[features]\n",
    "y = tfidf_df['is_serious']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Cross Val:  0.8637795275590551\n",
      "Best Params:  {'C': 10, 'max_iter': 150, 'solver': 'liblinear'}\n",
      "Train w/ Params:  0.9992125984251968\n",
      "Test w/ Params:  0.8301886792452831\n"
     ]
    }
   ],
   "source": [
    "# Gridsearch for Logistic Regression\n",
    "lr = LogisticRegression()\n",
    "lr_params = {\n",
    "    'solver': ['lbfgs', 'liblinear'],\n",
    "    'C' : [0.1,1,10,100],\n",
    "    'max_iter': [150]\n",
    "}\n",
    "gs = GridSearchCV(lr, param_grid=lr_params, cv=3)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# Scores\n",
    "print('Best Cross Val: ', gs.best_score_)\n",
    "print('Best Params: ', gs.best_params_)\n",
    "print('Train w/ Params: ', gs.score(X_train, y_train))\n",
    "print('Test w/ Params: ', gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obsevation: 0.1 improvement from CV before gridsearch, still overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest using Cross Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set X and y\n",
    "features = [column for column in cv_df.columns if column != 'is_serious']\n",
    "features.remove('sent_compound')\n",
    "features.remove('sent_neu')\n",
    "features.remove('char_count')\n",
    "X = cv_df[features]\n",
    "y = cv_df['is_serious']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Cross Val:  0.8125984251968504\n",
      "Best Params:  {'max_depth': 20, 'max_features': 2000, 'n_estimators': 150}\n",
      "Train w/ Params:  1.0\n",
      "Test w/ Params:  0.7688679245283019\n"
     ]
    }
   ],
   "source": [
    "# Gridsearch for Random Forest\n",
    "rf = RandomForestClassifier()\n",
    "rf_params = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth' : [10, 20, 30],\n",
    "    'max_features': ['auto', 1000, 2000]\n",
    "}\n",
    "gs = GridSearchCV(rf, param_grid=rf_params, cv=3) \n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# Scores\n",
    "print('Best Cross Val: ', gs.best_score_)\n",
    "print('Best Params: ', gs.best_params_)\n",
    "print('Train w/ Params: ', gs.score(X_train, y_train))\n",
    "print('Test w/ Params: ', gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations: Improved CV from 0.76 to 0.81 (but still equally as overfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Model: Logistic Regression with Tf-Idf and Grid Search parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import AdaBoost\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set X and y\n",
    "features = [column for column in tfidf_df.columns if column != 'is_serious']\n",
    "features.remove('sent_compound')\n",
    "features.remove('sent_neu')\n",
    "features.remove('char_count')\n",
    "X = tfidf_df[features]\n",
    "y = tfidf_df['is_serious']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Cross Val:  0.8582677165354331\n",
      "Best Params:  {'n_estimators': 80}\n",
      "Train w/ Params:  0.9165354330708662\n",
      "Test w/ Params:  0.8089622641509434\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression with Tf-Idf and Grid Search Paramters\n",
    "ada = AdaBoostClassifier(base_estimator=LogisticRegression(C = 10,\n",
    "                                                           max_iter = 150,\n",
    "                                                           solver = 'liblinear'))\n",
    "ada_params = {'n_estimators': [60, 70, 80]}\n",
    "                         \n",
    "gs = GridSearchCV(ada, param_grid=ada_params, cv=3)\n",
    "                         \n",
    "gs.fit(X_train, y_train);\n",
    "\n",
    "                         # Scores\n",
    "print('Best Cross Val: ', gs.best_score_)\n",
    "print('Best Params: ', gs.best_params_)\n",
    "print('Train w/ Params: ', gs.score(X_train, y_train))\n",
    "print('Test w/ Params: ', gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: AdaBoost did NOT improve my cross val score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SVD\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature matrix\n",
    "features = [column for column in tfidf_df.columns if column != 'is_serious']\n",
    "features.remove('sent_compound')\n",
    "features.remove('sent_neu')\n",
    "features.remove('char_count')\n",
    "X = tfidf_df[features]\n",
    "\n",
    "# Target\n",
    "y = tfidf_df['is_serious']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the SVD\n",
    "svd = TruncatedSVD(n_components=500)\n",
    "\n",
    "# Fit and transform\n",
    "svd_matrix = svd.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD Matrix Shape:     (1694, 500)\n",
      "Before SVD Shape:     (1694, 11251)\n",
      "SVD Components Shape: (500, 7922)\n"
     ]
    }
   ],
   "source": [
    "print('SVD Matrix Shape:    ', svd_matrix.shape)\n",
    "print('Before SVD Shape:    ', cv_df.shape)\n",
    "print('SVD Components Shape:', svd.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many components do we need to represent at least %70 of the variance in the data?\n",
    "np.where(np.cumsum(svd.explained_variance_ratio_) < .8)[0][-1]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>component_1</th>\n",
       "      <th>component_2</th>\n",
       "      <th>component_3</th>\n",
       "      <th>component_4</th>\n",
       "      <th>component_5</th>\n",
       "      <th>component_6</th>\n",
       "      <th>component_7</th>\n",
       "      <th>component_8</th>\n",
       "      <th>component_9</th>\n",
       "      <th>component_10</th>\n",
       "      <th>...</th>\n",
       "      <th>component_91</th>\n",
       "      <th>component_92</th>\n",
       "      <th>component_93</th>\n",
       "      <th>component_94</th>\n",
       "      <th>component_95</th>\n",
       "      <th>component_96</th>\n",
       "      <th>component_97</th>\n",
       "      <th>component_98</th>\n",
       "      <th>component_99</th>\n",
       "      <th>component_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.346687</td>\n",
       "      <td>0.182102</td>\n",
       "      <td>-0.011003</td>\n",
       "      <td>-0.020251</td>\n",
       "      <td>0.022137</td>\n",
       "      <td>-0.056799</td>\n",
       "      <td>-0.081593</td>\n",
       "      <td>-0.004827</td>\n",
       "      <td>-0.009376</td>\n",
       "      <td>-0.056244</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002169</td>\n",
       "      <td>-0.034651</td>\n",
       "      <td>-0.030947</td>\n",
       "      <td>-0.005019</td>\n",
       "      <td>-0.040476</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>0.049683</td>\n",
       "      <td>-0.016431</td>\n",
       "      <td>-0.043592</td>\n",
       "      <td>-0.003841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.382058</td>\n",
       "      <td>0.175217</td>\n",
       "      <td>-0.033306</td>\n",
       "      <td>0.030562</td>\n",
       "      <td>0.055140</td>\n",
       "      <td>-0.054848</td>\n",
       "      <td>0.029664</td>\n",
       "      <td>-0.031410</td>\n",
       "      <td>-0.087515</td>\n",
       "      <td>-0.084616</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039127</td>\n",
       "      <td>0.036918</td>\n",
       "      <td>-0.071137</td>\n",
       "      <td>-0.005108</td>\n",
       "      <td>0.049860</td>\n",
       "      <td>-0.016980</td>\n",
       "      <td>-0.007900</td>\n",
       "      <td>0.061943</td>\n",
       "      <td>0.066681</td>\n",
       "      <td>-0.056741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.167164</td>\n",
       "      <td>0.087790</td>\n",
       "      <td>-0.079841</td>\n",
       "      <td>0.032055</td>\n",
       "      <td>0.150531</td>\n",
       "      <td>-0.041592</td>\n",
       "      <td>-0.055154</td>\n",
       "      <td>0.099348</td>\n",
       "      <td>0.046849</td>\n",
       "      <td>0.390909</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012268</td>\n",
       "      <td>-0.005760</td>\n",
       "      <td>-0.018693</td>\n",
       "      <td>-0.020313</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>-0.014996</td>\n",
       "      <td>-0.004327</td>\n",
       "      <td>0.018752</td>\n",
       "      <td>0.014165</td>\n",
       "      <td>-0.018451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.224264</td>\n",
       "      <td>0.004795</td>\n",
       "      <td>-0.088161</td>\n",
       "      <td>-0.083690</td>\n",
       "      <td>0.075946</td>\n",
       "      <td>-0.046175</td>\n",
       "      <td>-0.034828</td>\n",
       "      <td>-0.148332</td>\n",
       "      <td>-0.117848</td>\n",
       "      <td>-0.033487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040306</td>\n",
       "      <td>-0.008616</td>\n",
       "      <td>-0.000884</td>\n",
       "      <td>-0.002848</td>\n",
       "      <td>0.048423</td>\n",
       "      <td>-0.016204</td>\n",
       "      <td>-0.003813</td>\n",
       "      <td>0.010586</td>\n",
       "      <td>0.040345</td>\n",
       "      <td>-0.018296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.310452</td>\n",
       "      <td>0.095704</td>\n",
       "      <td>-0.018119</td>\n",
       "      <td>0.011167</td>\n",
       "      <td>0.077147</td>\n",
       "      <td>0.010361</td>\n",
       "      <td>-0.050329</td>\n",
       "      <td>0.001458</td>\n",
       "      <td>-0.098694</td>\n",
       "      <td>-0.065023</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046591</td>\n",
       "      <td>-0.043783</td>\n",
       "      <td>-0.026082</td>\n",
       "      <td>0.034914</td>\n",
       "      <td>-0.030457</td>\n",
       "      <td>0.002014</td>\n",
       "      <td>-0.060900</td>\n",
       "      <td>0.002374</td>\n",
       "      <td>-0.011548</td>\n",
       "      <td>-0.032331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   component_1  component_2  component_3  component_4  component_5  \\\n",
       "0     0.346687     0.182102    -0.011003    -0.020251     0.022137   \n",
       "1     0.382058     0.175217    -0.033306     0.030562     0.055140   \n",
       "2     0.167164     0.087790    -0.079841     0.032055     0.150531   \n",
       "3     0.224264     0.004795    -0.088161    -0.083690     0.075946   \n",
       "4     0.310452     0.095704    -0.018119     0.011167     0.077147   \n",
       "\n",
       "   component_6  component_7  component_8  component_9  component_10  ...  \\\n",
       "0    -0.056799    -0.081593    -0.004827    -0.009376     -0.056244  ...   \n",
       "1    -0.054848     0.029664    -0.031410    -0.087515     -0.084616  ...   \n",
       "2    -0.041592    -0.055154     0.099348     0.046849      0.390909  ...   \n",
       "3    -0.046175    -0.034828    -0.148332    -0.117848     -0.033487  ...   \n",
       "4     0.010361    -0.050329     0.001458    -0.098694     -0.065023  ...   \n",
       "\n",
       "   component_91  component_92  component_93  component_94  component_95  \\\n",
       "0     -0.002169     -0.034651     -0.030947     -0.005019     -0.040476   \n",
       "1     -0.039127      0.036918     -0.071137     -0.005108      0.049860   \n",
       "2     -0.012268     -0.005760     -0.018693     -0.020313      0.002850   \n",
       "3      0.040306     -0.008616     -0.000884     -0.002848      0.048423   \n",
       "4     -0.046591     -0.043783     -0.026082      0.034914     -0.030457   \n",
       "\n",
       "   component_96  component_97  component_98  component_99  component_100  \n",
       "0      0.000873      0.049683     -0.016431     -0.043592      -0.003841  \n",
       "1     -0.016980     -0.007900      0.061943      0.066681      -0.056741  \n",
       "2     -0.014996     -0.004327      0.018752      0.014165      -0.018451  \n",
       "3     -0.016204     -0.003813      0.010586      0.040345      -0.018296  \n",
       "4      0.002014     -0.060900      0.002374     -0.011548      -0.032331  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coonvert to dataframe\n",
    "component_names = [\"component_\"+str(i+1) for i in range(100)]\n",
    "svd_df = pd.DataFrame(svd_matrix,\n",
    "                      columns=component_names)\n",
    "\n",
    "svd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Val Score:  0.857506470142755\n",
      "Train Score:      0.8842519685039371\n",
      "Test Score:       0.8089622641509434\n"
     ]
    }
   ],
   "source": [
    "# Try on the Logistic Regression model\n",
    "X_train, X_test, y_train, y_test = train_test_split(svd_df, y, stratify=y, random_state=42)\n",
    "\n",
    "lr = LogisticRegression(C = 10, max_iter = 150, solver = 'liblinear')\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "print('Cross Val Score: ', cross_val_score(lr, X_train, y_train, cv = 5).mean())\n",
    "print('Train Score:     ', lr.score(X_train, y_train))\n",
    "print('Test Score:      ', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations: The cross val score is very similar but our training and test score are much closer than previously (Train: 0.9992125984251968, Test: 0.8301886792452831) which means that the model is less overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model: Logistic Regression with Tf-Idf and SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------MODEL EVALUTATION---------------\n",
      "Cross Val Score:  0.857506470142755\n",
      "Train Score:      0.8842519685039371\n",
      "Test Score:       0.8089622641509434\n",
      "\n",
      "-------------CONFUSION MATRIX---------------\n",
      "            pred neg  pred pos\n",
      "actual neg       158        41\n",
      "actual pos        40       185\n",
      "\n",
      "True Negatives: 158\n",
      "False Positives: 41\n",
      "False Negatives: 40\n",
      "True Positives: 185\n",
      "------------------METRICS-------------------\n",
      "Sensativity: 0.8222\n",
      "Specificity: 0.794\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try on the Logistic Regression model\n",
    "X_train, X_test, y_train, y_test = train_test_split(svd_df, y, stratify=y, random_state=42)\n",
    "\n",
    "lr = LogisticRegression(C = 10, max_iter = 150, solver = 'liblinear')\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Score the model/Evaluate Model\n",
    "print('--------------MODEL EVALUTATION---------------')\n",
    "print('Cross Val Score: ', cross_val_score(lr, X_train, y_train, cv = 5).mean())\n",
    "print('Train Score:     ', lr.score(X_train, y_train))\n",
    "print('Test Score:      ', lr.score(X_test, y_test))\n",
    "print()\n",
    " \n",
    "# Get predictions\n",
    "predictions = lr.predict(X_test)\n",
    "    \n",
    "# Confusion matrix and measures\n",
    "confusion_matrix_results(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: \n",
    "- The model has false predictions pretty evenly\n",
    "- Almost the same number of false positives and negatives\n",
    "- Sensativity and specificty are pretty close"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
